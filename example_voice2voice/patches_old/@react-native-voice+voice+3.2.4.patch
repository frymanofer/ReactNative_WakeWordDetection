diff --git a/node_modules/@react-native-voice/voice/ios/Voice/Voice.m b/node_modules/@react-native-voice/voice/ios/Voice/Voice.m
index fd9dad8..79801ae 100644
--- a/node_modules/@react-native-voice/voice/ios/Voice/Voice.m
+++ b/node_modules/@react-native-voice/voice/ios/Voice/Voice.m
@@ -6,7 +6,19 @@
 #import <Speech/Speech.h>
 #import <Accelerate/Accelerate.h>
 
-
+/*
+
+| Feature / Property            | Purpose                                               |
+| ----------------------------- | ----------------------------------------------------- |
+| `addsPunctuation`             | Enables automatic punctuation in transcription        |
+| `shouldReportPartialResults`  | Interim partial transcripts for live updates          |
+| `requiresOnDeviceRecognition` | Forces on-device recognition only                     |
+| `contextualStrings`           | Language bias for custom words/phrases                |
+| `taskHint`                    | Language model tuning based on task intent            |
+| in `result`: speakingRate     | Tracks words-per-minute                               |
+| in `result.segments`          | Provides word-level timing, confidence, and analytics |
+
+*/
 @interface Voice () <SFSpeechRecognizerDelegate>
 
 @property (nonatomic) SFSpeechRecognizer* speechRecognizer;
@@ -33,14 +45,17 @@ @implementation Voice
 
 /** Returns "YES" if no errors had occurred */
 -(BOOL) setupAudioSession {
-    if ([self isHeadsetPluggedIn] || [self isHeadSetBluetooth]){
-        [self.audioSession setCategory:AVAudioSessionCategoryPlayAndRecord withOptions:AVAudioSessionCategoryOptionAllowBluetooth error: nil];
-    }
-    else {
-        [self.audioSession setCategory:AVAudioSessionCategoryPlayAndRecord withOptions:AVAudioSessionCategoryOptionDefaultToSpeaker error: nil];
-    }
+    [self.audioSession setCategory:AVAudioSessionCategoryPlayAndRecord
+                             mode:AVAudioSessionModeVoiceChat      // << voice‑chat mode
+              options:AVAudioSessionCategoryOptionAllowBluetooth |
+                            AVAudioSessionCategoryOptionAllowBluetoothA2DP |
+                            AVAudioSessionCategoryOptionDefaultToSpeaker //|
+ //                           AVAudioSessionCategoryOptionAllowAirPlay |
+ //                           AVAudioSessionCategoryOptionMixWithOthers
+                    error:nil];
     
     NSError* audioSessionError = nil;
+    [self.audioSession setPreferredSampleRate:48000 error:nil];     // VP‑IO runs at 48 k Hz
 
     // Activate the audio session
     [self.audioSession setActive:YES withOptions:AVAudioSessionSetActiveOptionNotifyOthersOnDeactivation error:&audioSessionError];
@@ -78,6 +93,40 @@ -(BOOL)isHeadSetBluetooth {
 
 - (void) teardown {
     self.isTearingDown = YES;
+
+    if (self.recognitionTask) {
+        [self.recognitionTask cancel];
+        self.recognitionTask = nil;
+    }
+
+    if (self.recognitionRequest) {
+        [self.recognitionRequest endAudio];
+        self.recognitionRequest = nil;
+    }
+
+    if (self.audioEngine) {
+        if (self.audioEngine.inputNode) {
+            [self.audioEngine.inputNode removeTapOnBus:0];
+            [self.audioEngine.inputNode reset];
+        }
+
+        if (self.audioEngine.isRunning) {
+            [self.audioEngine stop];
+        }
+
+        [self.audioEngine reset];
+        self.audioEngine = nil;  // Crucial step!
+    }
+
+    [self resetAudioSession];
+
+    self.sessionId = nil;
+    self.isTearingDown = NO;
+}
+
+
+- (void) teardown2 {
+    self.isTearingDown = YES;
     [self.recognitionTask cancel];
     self.recognitionTask = nil;
     
@@ -113,15 +162,50 @@ -(void) resetAudioSession {
     // Category hasn't changed -- do nothing
     if ([self.priorAudioCategory isEqualToString:audioCategory]) return;
     // Reset back to the previous category
-    if ([self isHeadsetPluggedIn] || [self isHeadSetBluetooth]) {
-        [self.audioSession setCategory:self.priorAudioCategory withOptions:AVAudioSessionCategoryOptionAllowBluetooth error: nil];
-    } else {
-        [self.audioSession setCategory:self.priorAudioCategory withOptions:AVAudioSessionCategoryOptionDefaultToSpeaker error: nil];
-    }
+    [self.audioSession setCategory:self.priorAudioCategory
+                  withOptions:AVAudioSessionCategoryOptionAllowBluetooth |
+                              AVAudioSessionCategoryOptionAllowBluetoothA2DP |
+                              AVAudioSessionCategoryOptionDefaultToSpeaker |
+                              AVAudioSessionCategoryOptionAllowAirPlay |
+                              AVAudioSessionCategoryOptionMixWithOthers
+                        error:nil];
     // Remove pointer reference
     self.audioSession = nil;
 }
 
+// Call once, right after you create the engine (or inside setupAudioSession)
+- (void)installEngineObservers
+{
+    NSNotificationCenter *nc = [NSNotificationCenter defaultCenter];
+
+    // Engine or any node reconfigures itself (often after a fatal render error)
+    [nc addObserver:self
+           selector:@selector(handleEngineConfigChange:)
+               name:AVAudioEngineConfigurationChangeNotification
+             object:self.audioEngine];
+
+    // Session interruptions also surface the same render errors
+    [nc addObserver:self
+           selector:@selector(handleSessionInterruption:)
+               name:AVAudioSessionInterruptionNotification
+             object:[AVAudioSession sharedInstance]];
+}
+
+- (void)handleSessionInterruption:(NSNotification *)note
+{
+    NSDictionary *info = note.userInfo;
+    AVAudioSessionInterruptionType type =
+        [info[AVAudioSessionInterruptionTypeKey] unsignedIntegerValue];
+
+    if (type == AVAudioSessionInterruptionTypeEnded) {
+        // On real “render err” Core Audio posts an interruption END
+        RCTLogWarn(@"Session interruption ended (possible render err):");
+    }
+}
+
+- (void)handleEngineConfigChange:(NSNotification *)notification {
+}
+
 - (void) setupAndStartRecognizing:(NSString*)localeStr {
     self.audioSession = [AVAudioSession sharedInstance];
     self.priorAudioCategory = [self.audioSession category];
@@ -148,8 +232,9 @@ - (void) setupAndStartRecognizing:(NSString*)localeStr {
         [self teardown];
         return;
     }
-    
+    [self installEngineObservers];
     self.recognitionRequest = [[SFSpeechAudioBufferRecognitionRequest alloc] init];
+    self.recognitionRequest.addsPunctuation = YES;
     // Configure request so that results are returned before audio recording is finished
     self.recognitionRequest.shouldReportPartialResults = YES;
     
@@ -164,16 +249,78 @@ - (void) setupAndStartRecognizing:(NSString*)localeStr {
     }
     
     @try {
-    AVAudioInputNode* inputNode = self.audioEngine.inputNode;
+    AVAudioInputNode *inputNode = self.audioEngine.inputNode;
     if (inputNode == nil) {
         [self sendResult:@{@"code": @"input"} :nil :nil :nil];
         [self teardown];
         return;
     }
+    // Enable voice processing
+    NSError *error = nil;
+    if (![self.audioEngine.inputNode setVoiceProcessingEnabled:YES error:&error]) {
+        NSLog(@"Failed to enable voice processing for AEC on input node: %@", error);
+    }
+    if (![self.audioEngine.outputNode setVoiceProcessingEnabled:YES error:&error]) {
+        NSLog(@"Failed to enable voice processing for AEC on output node: %@", error);
+    }
+
     
     [self sendEventWithName:@"onSpeechStart" body:nil];
     
+    NSLog(@"After Enable voice processing");
+    AVAudioFormat* recordingFormat = 
+            [inputNode outputFormatForBus:0];
+    //AVAudioMixerNode *mixer = [[AVAudioMixerNode alloc] init];
+    //[self.audioEngine attachNode:mixer];
+    //[self.audioEngine connect:inputNode to:mixer format:recordingFormat];
+    [self.audioEngine connect:inputNode to:self.audioEngine.mainMixerNode format:recordingFormat];
+    [self.audioEngine connect:self.audioEngine.mainMixerNode to:self.audioEngine.outputNode format:recordingFormat];
+    // silence output to avoid hearing mic
+    self.audioEngine.mainMixerNode.outputVolume = 0.0;
+    NSLog(@"After Connect");
+    // Start recording and append recording buffer to speech recognizer
+    @try {
+        [inputNode installTapOnBus:0 bufferSize:1024 format:recordingFormat block:^(AVAudioPCMBuffer * _Nonnull buffer, AVAudioTime * _Nonnull when) {
+        // [mixer installTapOnBus:0 bufferSize:1024 format:recordingFormat block:^(AVAudioPCMBuffer * _Nonnull buffer, AVAudioTime * _Nonnull when) {
+            //Volume Level Metering
+            UInt32 inNumberFrames = buffer.frameLength;
+            float LEVEL_LOWPASS_TRIG = 0.5;
+            if(buffer.format.channelCount>0)
+            {
+                Float32* samples = (Float32*)buffer.floatChannelData[0];
+                Float32 avgValue = 0;
+
+                vDSP_maxmgv((Float32*)samples, 1, &avgValue, inNumberFrames);
+                self.averagePowerForChannel0 = (LEVEL_LOWPASS_TRIG*((avgValue==0)?-100:20.0*log10f(avgValue))) + ((1-LEVEL_LOWPASS_TRIG)*self.averagePowerForChannel0) ;
+                self.averagePowerForChannel1 = self.averagePowerForChannel0;
+            }
+
+            if(buffer.format.channelCount>1)
+            {
+                Float32* samples = (Float32*)buffer.floatChannelData[1];
+                Float32 avgValue = 0;
+
+                vDSP_maxmgv((Float32*)samples, 1, &avgValue, inNumberFrames);
+                self.averagePowerForChannel1 = (LEVEL_LOWPASS_TRIG*((avgValue==0)?-100:20.0*log10f(avgValue))) + ((1-LEVEL_LOWPASS_TRIG)*self.averagePowerForChannel1) ;
+
+            }
+            // Normalizing the Volume Value on scale of (0-10)
+            self.averagePowerForChannel1 = [self _normalizedPowerLevelFromDecibels:self.averagePowerForChannel1]*10;
+            NSNumber *value = [NSNumber numberWithFloat:self.averagePowerForChannel1];
+            [self sendEventWithName:@"onSpeechVolumeChanged" body:@{@"value": value}];
             
+            // Todo: write recording buffer to file (if user opts in)
+            if (self.recognitionRequest != nil) {
+                [self.recognitionRequest appendAudioPCMBuffer:buffer];
+            }
+        }];
+    } @catch (NSException *exception) {
+        NSLog(@"[Error] - %@ %@", exception.name, exception.reason);
+        [self sendResult:@{@"code": @"start_recording", @"message": [exception reason]} :nil :nil :nil];
+        [self teardown];
+        return;
+    } @finally {}
+    NSLog(@"Before Enable voice processing");
     // A recognition task represents a speech recognition session.
     // We keep a reference to the task so that it can be cancelled.
     NSString *taskSessionId = self.sessionId;
@@ -215,65 +362,33 @@ - (void) setupAndStartRecognizing:(NSString*)localeStr {
         }
         
     }];
+//    /*
+//*/
     
-    AVAudioFormat* recordingFormat = [inputNode outputFormatForBus:0];
-    AVAudioMixerNode *mixer = [[AVAudioMixerNode alloc] init];
-    [self.audioEngine attachNode:mixer];
-    
-    // Start recording and append recording buffer to speech recognizer
-    @try {
-        [mixer installTapOnBus:0 bufferSize:1024 format:recordingFormat block:^(AVAudioPCMBuffer * _Nonnull buffer, AVAudioTime * _Nonnull when) {
-            //Volume Level Metering
-            UInt32 inNumberFrames = buffer.frameLength;
-            float LEVEL_LOWPASS_TRIG = 0.5;
-            if(buffer.format.channelCount>0)
-            {
-                Float32* samples = (Float32*)buffer.floatChannelData[0];
-                Float32 avgValue = 0;
-
-                vDSP_maxmgv((Float32*)samples, 1, &avgValue, inNumberFrames);
-                self.averagePowerForChannel0 = (LEVEL_LOWPASS_TRIG*((avgValue==0)?-100:20.0*log10f(avgValue))) + ((1-LEVEL_LOWPASS_TRIG)*self.averagePowerForChannel0) ;
-                self.averagePowerForChannel1 = self.averagePowerForChannel0;
-            }
-
-            if(buffer.format.channelCount>1)
-            {
-                Float32* samples = (Float32*)buffer.floatChannelData[1];
-                Float32 avgValue = 0;
-
-                vDSP_maxmgv((Float32*)samples, 1, &avgValue, inNumberFrames);
-                self.averagePowerForChannel1 = (LEVEL_LOWPASS_TRIG*((avgValue==0)?-100:20.0*log10f(avgValue))) + ((1-LEVEL_LOWPASS_TRIG)*self.averagePowerForChannel1) ;
-
-            }
-            // Normalizing the Volume Value on scale of (0-10)
-            self.averagePowerForChannel1 = [self _normalizedPowerLevelFromDecibels:self.averagePowerForChannel1]*10;
-            NSNumber *value = [NSNumber numberWithFloat:self.averagePowerForChannel1];
-            [self sendEventWithName:@"onSpeechVolumeChanged" body:@{@"value": value}];
-            
-            // Todo: write recording buffer to file (if user opts in)
-            if (self.recognitionRequest != nil) {
-                [self.recognitionRequest appendAudioPCMBuffer:buffer];
-            }
-        }];
-    } @catch (NSException *exception) {
-        NSLog(@"[Error] - %@ %@", exception.name, exception.reason);
-        [self sendResult:@{@"code": @"start_recording", @"message": [exception reason]} :nil :nil :nil];
-        [self teardown];
-        return;
-    } @finally {}
+    NSLog(@"After Start recording and append recording");
 
-    [self.audioEngine connect:inputNode to:mixer format:recordingFormat];
     [self.audioEngine prepare];
+    NSLog(@"audioEngine prepare");
     NSError* audioSessionError = nil;
     [self.audioEngine startAndReturnError:&audioSessionError];
+    NSLog(@"audioEngine startAndReturnError");
     if (audioSessionError != nil) {
+        [[NSNotificationCenter defaultCenter] addObserver:self
+                                            selector:@selector(handleEngineConfigChange:)
+                                                name:AVAudioEngineConfigurationChangeNotification
+                                            object:self.audioEngine];
+        NSLog(@"audioEngine audioSessionError!=nil");
         [self sendResult:@{@"code": @"audio", @"message": [audioSessionError localizedDescription]} :nil :nil :nil];
+        NSLog(@"self sendResult");
         [self teardown];
+        NSLog(@"self teardown");
         return;
     }
+    NSLog(@"After if audioSessionError != nil");
     }
     @catch (NSException *exception) {
     [self sendResult:@{@"code": @"start_recording", @"message": [exception reason]} :nil :nil :nil];
+    NSLog(@"End of init...");
     return;
   }
 }
